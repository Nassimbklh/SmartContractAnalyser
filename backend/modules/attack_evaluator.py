"""
Attack Evaluation Module
Handles evaluation of attack effectiveness using reward models
"""

import json
import re
from typing import Dict, Any, Tuple
import openai
from .attack_generator import DecimalEncoder


def log(msg: str):
    """
    Logs a given message to the console.

    This function accepts a string message and prints it to the standard
    output. It is used for direct and simple logging purposes.

    :param msg: The message that needs to be logged. It should be a string.
    :type msg: str
    :return: None
    """
    print(msg)


def reward_prompt(observation: Dict[str, Any], llm_response: str, attack_result: Dict[str, Any]) -> str:
    """
    Constructs a detailed prompt for evaluating the effectiveness and relevance of an attack
    attempt on a smart contract. The function generates a formatted string that includes
    context from the provided data, such as the observation, the response from the large
    language model, and the attack result. The goal is to assess, in a structured manner,
    how critical the exploit is and provide reasoning for the evaluation.

    :param observation: Dictionary containing the contracts, states, or other data relevant to
        the context of the attack. This data provides the basis for evaluating the attack's
        implications.
    :type observation: Dict[str, Any]
    :param llm_response: Textual response generated by the large language model (LLM), which
        may provide insights or highlight aspects of the attack.
    :type llm_response: str
    :param attack_result: Dictionary with details about the outcome or implications of the
        attack, including any discovered vulnerabilities or outcomes of simulated actions.
    :type attack_result: Dict[str, Any]
    :return: A formatted string that provides a concise, structured, and detailed prompt for
        assessing the attack. It requests a numerical score for the attack's criticality and
        a justification for the provided score.
    :rtype: str
    """
    prompt = f"""
You are an advanced smart contract auditor. Evaluate the following attack attempt.

CONTEXT:
- Contracts and states (truncated): {json.dumps(observation, indent=2, cls=DecimalEncoder)[:2000]}
- LLM (Codestral) response: {llm_response[:1000]}
- Attack result: {json.dumps(attack_result)}

Please rate the effectiveness and relevance of this attack on a scale from 0 (useless) to 10 (critical exploit), and justify your rating.
Format: SCORE: <number> | COMMENT: <reasoning>
"""
    return prompt


def parse_reward_output(reward_output: str) -> Tuple[float, str]:
    """
    Parses the given reward output string to extract the numerical score and textual
    comment. The reward output follows a specific format, containing a "SCORE"
    section with a numeric value and a "COMMENT" section with relevant text. The
    function uses regular expressions to extract and return these values.

    :param reward_output: The input string containing reward-related information,
                          including SCORE and COMMENT sections.
    :type reward_output: str
    :return: A tuple where the first element is the extracted numeric score as a
             float, and the second element is the extracted comment as a string.
    :rtype: Tuple[float, str]
    """
    score = 0.0
    comment = ""

    try:
        # Extract score
        score_match = re.search(r'SCORE\s*:\s*([\d.]+)', reward_output, re.IGNORECASE)
        if score_match:
            score = float(score_match.group(1))

        # Extract comment
        comment_match = re.search(r'COMMENT\s*:\s*(.*)', reward_output, re.IGNORECASE)
        if comment_match:
            comment = comment_match.group(1).strip()

    except Exception as e:
        log(f"Error parsing reward output: {e}")

    return score, comment


def query_gpt4_reward(prompt: str, model: str = "gpt-4.1-mini", temperature: float = 0) -> str:
    """
    Execute a query to the GPT-4 reward model and retrieve its response. The function
    interacts with the OpenAI API to send a prompt to the specified model and returns
    the generated response text. If an error occurs during the API call, it logs the error
    message and returns an error string.

    :param prompt: The text input provided by the user to query the reward model.
    :type prompt: str
    :param model: The identifier of the GPT-4 model to use for the query.
    :type model: str
    :param temperature: The level of randomness in the output generation; a higher value
        encourages more diversity, while a lower value makes it more deterministic.
    :type temperature: float
    :return: The response content generated by the reward model or an error string if an
        exception is raised during the API call.
    :rtype: str
    """
    try:
        response = openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=250,
            stop=None,
        )
        return response.choices[0].message.content

    except Exception as e:
        log(f"Error querying reward model: {e}")
        return f"ERROR: {e}"


def evaluate_attack(observation: Dict[str, Any], llm_response: str, attack_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Evaluate the attack by constructing a reward prompt, querying a reward model,
    and parsing the model's output to return detailed evaluation metrics.

    The function takes in an observation of the current state, a response generated
    by the language model (LLM), and the result of the simulated attack scenario.
    Using these inputs, it constructs a reward prompt and uses an external
    reward model (e.g., GPT-4) to evaluate the performance of the attack. The
    resulting evaluation includes a raw reward model output, a parsed reward score,
    and additional reward commentary for further analysis.

    :param observation: Input data representing the current state of the system
        under evaluation.
    :type observation: Dict[str, Any]
    :param llm_response: The textual response generated by the language model
        during the attack scenario.
    :type llm_response: str
    :param attack_result: The result of the simulated attack scenario, including
        relevant details and outcomes.
    :type attack_result: Dict[str, Any]
    :return: A dictionary containing the reward prompt text, the raw output
        from the reward model, the parsed reward score, and the reward comments.
    :rtype: Dict[str, Any]
    """
    # Build reward prompt
    reward_prompt_text = reward_prompt(observation, llm_response, attack_result)

    # Query reward model
    reward_raw = query_gpt4_reward(reward_prompt_text)

    # Parse reward output
    reward_score, reward_comment = parse_reward_output(reward_raw)

    return {
        "reward_prompt": reward_prompt_text,
        "reward_raw_output": reward_raw,
        "reward_score": reward_score,
        "reward_comment": reward_comment
    }
